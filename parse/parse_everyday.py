import asyncio
import multiprocessing
from typing import List, Dict

from asyncpg import Record

from config import logger
from database.events_db import add_events, add_descriptions, get_events_without_description
from parse.afisha.parse_events import get_all_events
from parse.afisha.parse_description_of_events import get_descriptions

def run_parallel(urls: List[str], num_processes: int = 3) -> Dict[str, str]:
    """
    –ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–∞—Ä—Å–∏–Ω–≥ –≤ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã.
    """
    chunk_size = max(1, len(urls) // num_processes)  # –†–∞–∑–±–∏–≤–∞–µ–º —Å–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫ –Ω–∞ —á–∞—Å—Ç–∏
    url_chunks = [urls[i:i + chunk_size] for i in range(0, len(urls), chunk_size)]

    logger.info(f"üîÑ –ó–∞–ø—É—Å–∫–∞–µ–º {num_processes} –ø—Ä–æ—Ü–µ—Å—Å–æ–≤, –∫–∞–∂–¥–∞—è —á–∞—Å—Ç—å —Å–æ–¥–µ—Ä–∂–∏—Ç {chunk_size} —Å—Å—ã–ª–æ–∫...")

    with multiprocessing.Pool(processes=num_processes) as pool:
        results = pool.starmap(get_descriptions, [(i, chunk) for i, chunk in enumerate(url_chunks)])
        # results = pool.map(get_descriptions, url_chunks)

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—Å–µ—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –≤ –æ–¥–∏–Ω —Å–ª–æ–≤–∞—Ä—å
    merged_results = {}
    for result in results:
        merged_results.update(result)

    return merged_results


async def parse_everyday_afisha():
    # all_events_list_of_dicts = get_all_events()
    link_of_events = []

    # if all_events_list_of_dicts is not None:
    #     await add_events(all_events_list_of_dicts)

    list_of_records = await get_events_without_description()
    list_of_links = [record['source'] for record in list_of_records]

    if list_of_links is not None:
        description = run_parallel(list_of_links)
        await add_descriptions(description)


if __name__ == "__main__":
    asyncio.run(parse_everyday_afisha())